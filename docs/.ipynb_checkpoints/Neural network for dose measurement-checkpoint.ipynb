{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd6f36b0",
   "metadata": {},
   "source": [
    "# Neural network for dose measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d031b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib tk\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import NNPytorchLightning as NNPL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a04e66",
   "metadata": {},
   "source": [
    "Here loading trained models to compare the training and validation losses per model, and look at performance on unseen data. These models are as follows:\n",
    "\n",
    "model_cubes: trained just on cube counts, with a batch size of 200 and 200 samples per data set, learning rate of 1e-3, for 500 epochs (for time)\n",
    "\n",
    "model_cubes_profiles: trained on cube counts and profiles, with a batch size of 200 and 200 samples per data set, learning rate of 1e-3, for 500 epochs (for time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a47ea1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs = '/home/nr1315/Documents/Project/effective_dose_coeffs.h5'\n",
    "energy_bins = '/home/nr1315/Documents/Project/MachineLearning/energy_bins.npy'\n",
    "\n",
    "model_cubes = NNPL.LoadModel('/home/nr1315/Documents/Project/MachineLearning/lightning_logs/model_cubes_new_data/version_1/',torch.rand((1,1,64)),coeffs,energy_bins)\n",
    "\n",
    "model_cubes_profiles = NNPL.LoadModel('/home/nr1315/Documents/Project/MachineLearning/lightning_logs/model_cubes_profiles_new_data/version_5/',torch.rand((1,1,76)),coeffs,energy_bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bed94dd",
   "metadata": {},
   "source": [
    "Also need to load the loss curves separately, due to the way they are from the logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a405991",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cubes_tloss = pd.read_csv('/home/nr1315/Downloads/run-model_cubes_new_data_version_1-tag-train_loss.csv')\n",
    "model_cubes_vloss = pd.read_csv('/home/nr1315/Downloads/run-model_cubes_new_data_version_1-tag-val_loss.csv')\n",
    "\n",
    "model_cubes_profiles_tloss = pd.read_csv('/home/nr1315/Downloads/run-model_cubes_profiles_new_data_version_5-tag-train_loss.csv')\n",
    "model_cubes_profiles_vloss = pd.read_csv('/home/nr1315/Downloads/run-model_cubes_profiles_new_data_version_5-tag-val_loss.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9939c8bc",
   "metadata": {},
   "source": [
    "We also define the directories from which to load the testing data, for convenience:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "999beda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data_dir = '/home/nr1315/Documents/Project/MachineLearning/TestingData/'\n",
    "\n",
    "AmBe_counts = 'SimCubeCounts_AmBe_5_0-0-0-0-1-0_1500.npy'\n",
    "AmLi_counts = 'SimCubeCounts_AmLi_5_0-0-0-0-1-0_1500.npy'\n",
    "Cf_counts = 'SimCubeCounts_Cf252_6_0-0-0-0-1-0_1500.npy'\n",
    "\n",
    "AmBe_target = 'SimEnergyBins_AmBe.npy'\n",
    "AmLi_target = 'SimEnergyBins_AmLi.npy'\n",
    "Cf_target = 'SimEnergyBins_Cf252.npy'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9908db3",
   "metadata": {},
   "source": [
    "For each model in turn, we will look at the loss curves and prediction on AmBe, AmLi, and Cf-252. \n",
    "\n",
    "### Cubes only model, 500 epochs, 200 samples per dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f27b4824",
   "metadata": {},
   "outputs": [],
   "source": [
    "NNPL.PlotLosses([model_cubes_tloss,model_cubes_vloss],['Training loss','Validation loss'],'Model with only cube counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505dd294",
   "metadata": {},
   "source": [
    "![Loss curves](Plots/model_cubes_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1e4328",
   "metadata": {},
   "source": [
    "Here the model seems to be training well, although it does not appear to finish training after 500 epochs. \n",
    "\n",
    "We now look at the model prediction quality on some unseen data, using the compare_pred_true function. This also returns the testing loss of the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ce27fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,3,figsize=(30,10))\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "l1 = NNPL.compare_pred_true(model_cubes,testing_data_dir+AmBe_counts,testing_data_dir+AmBe_target,'AmBe','cubes only model',ax[0],24,np.load(energy_bins),1,loss_fn)\n",
    "l2 = NNPL.compare_pred_true(model_cubes,testing_data_dir+AmLi_counts,testing_data_dir+AmLi_target,'AmLi','cubes only model',ax[1],24,np.load(energy_bins),1,loss_fn)\n",
    "l3 = NNPL.compare_pred_true(model_cubes,testing_data_dir+Cf_counts,testing_data_dir+Cf_target,r'$^{252}$Cf','cubes only model',ax[2],24,np.load(energy_bins),1,loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238cd8d2",
   "metadata": {},
   "source": [
    "![Predictions of test data](Plots/model_cubes_prediction.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bcc7e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AmBe loss: 0.0878705158829689\n",
      "AmLi loss: 0.04326992481946945\n",
      "Cf-252 loss: 0.017681293189525604\n"
     ]
    }
   ],
   "source": [
    "print(\"AmBe loss: {}\".format(l1))\n",
    "print(\"AmLi loss: {}\".format(l2))\n",
    "print(\"Cf-252 loss: {}\".format(l3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fe0286",
   "metadata": {},
   "source": [
    "Whilst the model loss continues to decrease, the model still has poor performance on the unseen sources,  predicting negative values in at least half the bins in all three cases. It generally seems to predict the greatest count in the region around the average energy of each source, i.e. above, below and at 1 MeV for AmBe, AmLi and Cf respectively, as previous iterations of the model did. One possible way to combat this may be to introduce more complex sources into the training set, such as linear combinations of existing monoenergetics, to help the network learn how to better reconstruct a more complicated fluence.\n",
    "\n",
    "It is then informative to see the performance of the model on some of the validation data explicitly, as is shown below. The function used to plot this can be used to scroll through all of the training datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79942c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from NNPytorchLightning import FluenceReconDataset, Resample\n",
    "\n",
    "cubes_dataloader = torch.load('/home/nr1315/Documents/Project/MachineLearning/lightning_logs/model_cubes_new_data/version_1/val_dloader.pt')\n",
    "cubes_dataset,cubes_val_inds = cubes_dataloader.dataset.dataset,cubes_dataloader.dataset.indices\n",
    "\n",
    "check = NNPL.CheckTrainData(model_cubes,cubes_dataset,np.load(energy_bins))\n",
    "\n",
    "check.ViewTrainData()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548bce26",
   "metadata": {},
   "source": [
    "![550keV model cubes pred](Plots/model_cubes_550keV_pred.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f532f0",
   "metadata": {},
   "source": [
    "The model has generally performed well at predicting the bins for this validation data point with some count in an incorrect bin, but of note is that it is still predicting negative values in several bins. In order to avoid this, it may prove useful to add a term to the loss function that penalises any negative bins in the model prediction. \n",
    "\n",
    "A more thorough check of the validation data set for both this model and the following will be performed later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec887dc",
   "metadata": {},
   "source": [
    "### Cubes and profiles model, 200 samples per dataset, learning rate 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d33d5ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "NNPL.PlotLosses([model_cubes_profiles_tloss,model_cubes_profiles_vloss],['Training loss','Validation loss'],'Cubes and profiles model, lr = 0.001, 200 samples per dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c15676b",
   "metadata": {},
   "source": [
    "![losses](Plots/model_cubes_profiles_200samples.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0ee180",
   "metadata": {},
   "source": [
    "Whilst both the training loss and validation loss do both decrease, it is worth noting that the validation loss here is lower than the training loss, which in turn implies that the model needs to train for longer in this configuration. This is intuitive, as the increased number of bins due to the profile counts will mean there are a greater number of weights for the model to optimize. Other improvements may be found from increasing the learning rate or adding some learning rate scheduling, but otherwise training for longer is realistically required.\n",
    "\n",
    "It is also worthy of note that the loss is higher than for the cubes-only model, but this may be a factor of the incomplete training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c20cae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,3,figsize=(30,10))\n",
    "\n",
    "l4 = NNPL.compare_pred_true(model_cubes_profiles,testing_data_dir+'withProfiles/'+AmBe_counts,testing_data_dir+AmBe_target,'AmBe','cubes and \\n profiles model',ax[0],24,np.load(energy_bins),1,loss_fn)\n",
    "l5 = NNPL.compare_pred_true(model_cubes_profiles,testing_data_dir+'withProfiles/'+AmLi_counts,testing_data_dir+AmLi_target,'AmLi','cubes and \\n profiles model',ax[1],24,np.load(energy_bins),1,loss_fn)\n",
    "l6 = NNPL.compare_pred_true(model_cubes_profiles,testing_data_dir+'withProfiles/'+Cf_counts,testing_data_dir+Cf_target,r'$^{252}$Cf','cubes and \\n profiles model',ax[2],24,np.load(energy_bins),1,loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c48bee",
   "metadata": {},
   "source": [
    "![cubes profiles 200 samples prediction](Plots/model_cubes_profiles_200samples_prediction.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6c1b932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AmBe loss: 0.09213663637638092\n",
      "AmLi loss: 0.05291319638490677\n",
      "Cf-252 loss: 0.04942954331636429\n"
     ]
    }
   ],
   "source": [
    "print(\"AmBe loss: {}\".format(l4))\n",
    "print(\"AmLi loss: {}\".format(l5))\n",
    "print(\"Cf-252 loss: {}\".format(l6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba312d44",
   "metadata": {},
   "source": [
    "This model also performs poorly on the unseen data, although it has a lower loss than the cubes exclusive model. Most notably, this model predicts a large negative value in the first bin for all three sources, which further motivates introducing a penalty term to the loss to discourage any negative values in the model output. \n",
    "\n",
    "\n",
    "Validation data set checking needs some more work before conclusions can be drawn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "377b0678",
   "metadata": {},
   "outputs": [],
   "source": [
    "cubes_profiles_dataloader = torch.load('/home/nr1315/Documents/Project/MachineLearning/lightning_logs/model_cubes_profiles_new_data/version_5/val_dloader.pt')\n",
    "cubes_profiles_dataset,cubes_profiles_val_inds = cubes_profiles_dataloader.dataset.dataset,cubes_profiles_dataloader.dataset.indices\n",
    "\n",
    "check = NNPL.CheckTrainData(model_cubes_profiles,cubes_profiles_dataset,np.load(energy_bins))\n",
    "\n",
    "check.ViewTrainData()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
