{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd6f36b0",
   "metadata": {},
   "source": [
    "# Neural network for dose measurement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81434f7f",
   "metadata": {},
   "source": [
    "### Introduction and motivation\n",
    "In modern dosimetry, the gold standard of radiation dose measurement is _effective dose_. This quantity is defined as a sum over dose to each major organ in the human body, weighted by a set of _tissue weighting factors_ that encode the average risk to health of a dose to that organ. This is a complex and difficult measurement to make, as many modern dosimeters are either optimised for field direction measurement or a fluence measurement. For example, for a Bonner sphere spectrometer, measuring effective dose is a time-consuming and computationally intensive process that requires careful measurement of the radiation field with varying size of polyethylene sphere and a complex unfolding algorithm to reconstruct the fluence, and this still lacks directional information about the field. \n",
    "\n",
    "In contrast, the segmentation of nFacet 3D encodes both the direction and the energy of an incident neutron field. This can be visualised through the direction and intensity of attenuation of neutron count in the detector cubes. There are therefore two components to measuring the effective dose: reconstructing the fluence of the incident neutron field, and the direction of incidence of the neutrons. Here I focus on the fluence reconstruction using an artificial neural network (ANN), as once trained this is a fast method for reconstructing the fluence without need for a complex unfolding algorithm. \n",
    "\n",
    "### Neural network architecture\n",
    "\n",
    "The information for the network to learn is encoded in the distribution of neutron count across cubes in the detector. As a result, the first choice of input into the network consisted of 64 input neurons corresponding to the 64 cubes of the detector, whilst the output of the ANN was the binned fluence of the source. Additionally, summing the counts in planes of cubes provides additional information about the bulk response of the detector to the applied field and thus encodes more detailed information about the energy of the incident neutrons. This can be added to the inputs, adding 12 additional neurons corresponding to the four planes in the x, y and z directions. The model has subsequently been trained with and without these additional inputs to evaluate performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc316b4d",
   "metadata": {},
   "source": [
    "### Fluence binning scheme\n",
    "\n",
    "TO BE ADDED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d031b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib tk\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import NNPytorchLightning as NNPL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a04e66",
   "metadata": {},
   "source": [
    "Here loading trained models to compare the training and validation losses per model, and look at performance on unseen data. These models are as follows:\n",
    "\n",
    "model_cubes: trained just on cube counts, with a batch size of 200 and 200 samples per data set, learning rate of 1e-3, for 500 epochs (for time)\n",
    "\n",
    "model_cubes_profiles: trained on cube counts and profiles, with a batch size of 200 and 200 samples per data set, learning rate of 1e-3, for 500 epochs (for time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a47ea1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs = '/home/nr1315/Documents/Project/effective_dose_coeffs.h5'\n",
    "energy_bins = '/home/nr1315/Documents/Project/MachineLearning/energy_bins.npy'\n",
    "\n",
    "model_cubes = NNPL.LoadModel('/home/nr1315/Documents/Project/MachineLearning/lightning_logs/model_cubes_new_data/version_1/',torch.rand((1,1,64)),coeffs,energy_bins)\n",
    "\n",
    "model_cubes_profiles = NNPL.LoadModel('/home/nr1315/Documents/Project/MachineLearning/lightning_logs/model_cubes_profiles_new_data/version_5/',torch.rand((1,1,76)),coeffs,energy_bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bed94dd",
   "metadata": {},
   "source": [
    "Also need to load the loss curves separately, due to the way they are from the logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a405991",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cubes_tloss = pd.read_csv('/home/nr1315/Documents/Project/MachineLearning/LoggedParameters/run-model_cubes_new_data_version_1-tag-train_loss.csv')\n",
    "model_cubes_vloss = pd.read_csv('/home/nr1315/Documents/Project/MachineLearning/LoggedParameters/run-model_cubes_new_data_version_1-tag-val_loss.csv')\n",
    "model_cubes_dose_err = pd.read_csv('/home/nr1315/Documents/Project/MachineLearning/LoggedParameters/run-model_cubes_new_data_version_1-tag-dose_err_AP.csv')\n",
    "model_cubes_epoch = pd.read_csv('/home/nr1315/Documents/Project/MachineLearning/LoggedParameters/run-model_cubes_new_data_version_1-tag-epoch.csv')\n",
    "\n",
    "model_cubes_profiles_tloss = pd.read_csv('/home/nr1315/Documents/Project/MachineLearning/LoggedParameters/run-model_cubes_profiles_new_data_version_5-tag-train_loss.csv')\n",
    "model_cubes_profiles_vloss = pd.read_csv('/home/nr1315/Documents/Project/MachineLearning/LoggedParameters/run-model_cubes_profiles_new_data_version_5-tag-val_loss.csv')\n",
    "model_cubes_profiles_dose_err = pd.read_csv('/home/nr1315/Documents/Project/MachineLearning/LoggedParameters/run-model_cubes_profiles_new_data_version_5-tag-dose_err_AP.csv')\n",
    "model_cubes_profiles_epoch = pd.read_csv('/home/nr1315/Documents/Project/MachineLearning/LoggedParameters/run-model_cubes_profiles_new_data_version_5-tag-epoch.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9939c8bc",
   "metadata": {},
   "source": [
    "We also define the directories from which to load the testing data, for convenience:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "999beda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data_dir = '/home/nr1315/Documents/Project/MachineLearning/TestingData/'\n",
    "\n",
    "AmBe_counts = 'SimCubeCounts_AmBe_5_0-0-0-0-1-0_1500.npy'\n",
    "AmLi_counts = 'SimCubeCounts_AmLi_5_0-0-0-0-1-0_1500.npy'\n",
    "Cf_counts = 'SimCubeCounts_Cf252_6_0-0-0-0-1-0_1500.npy'\n",
    "\n",
    "AmBe_target = 'SimEnergyBins_AmBe.npy'\n",
    "AmLi_target = 'SimEnergyBins_AmLi.npy'\n",
    "Cf_target = 'SimEnergyBins_Cf252.npy'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9908db3",
   "metadata": {},
   "source": [
    "For each model in turn, we will look at the loss curves and prediction on AmBe, AmLi, and Cf-252. \n",
    "\n",
    "### Cubes only model, 500 epochs, 200 samples per dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f27b4824",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nr1315/miniconda3/envs/NewMLEnv/lib/python3.9/site-packages/scipy/interpolate/interpolate.py:623: RuntimeWarning: invalid value encountered in true_divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/nr1315/miniconda3/envs/NewMLEnv/lib/python3.9/site-packages/scipy/interpolate/interpolate.py:623: RuntimeWarning: invalid value encountered in true_divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n"
     ]
    }
   ],
   "source": [
    "NNPL.PlotLosses([model_cubes_tloss,model_cubes_vloss],['Training loss','Validation loss'],'Cubes model, lr = 0.001, 200 samples per dataset',model_cubes_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbaf6301",
   "metadata": {},
   "source": [
    "![Loss curves](Plots/model_cubes_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1e4328",
   "metadata": {},
   "source": [
    "Here the model seems to be training well, although it does not appear to finish training after 500 epochs. \n",
    "\n",
    "It is also informative to look at the quality of model prediction on unseen data, in this case AmLi, AmBe, and Cf252 sources. These can be seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ce27fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,3,figsize=(30,10))\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "l1 = NNPL.compare_pred_true(model_cubes,testing_data_dir+AmBe_counts,testing_data_dir+AmBe_target,'AmBe','cubes only model',ax[0],24,np.load(energy_bins),1,loss_fn)\n",
    "l2 = NNPL.compare_pred_true(model_cubes,testing_data_dir+AmLi_counts,testing_data_dir+AmLi_target,'AmLi','cubes only model',ax[1],24,np.load(energy_bins),1,loss_fn)\n",
    "l3 = NNPL.compare_pred_true(model_cubes,testing_data_dir+Cf_counts,testing_data_dir+Cf_target,r'$^{252}$Cf','cubes only model',ax[2],24,np.load(energy_bins),1,loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238cd8d2",
   "metadata": {},
   "source": [
    "![Predictions of test data](Plots/model_cubes_prediction.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bcc7e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AmBe loss: 0.0878705158829689\n",
      "AmLi loss: 0.04326992481946945\n",
      "Cf-252 loss: 0.017681293189525604\n"
     ]
    }
   ],
   "source": [
    "print(\"AmBe loss: {}\".format(l1))\n",
    "print(\"AmLi loss: {}\".format(l2))\n",
    "print(\"Cf-252 loss: {}\".format(l3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fe0286",
   "metadata": {},
   "source": [
    "Whilst the model loss continues to decrease, the model still has poor performance on the unseen sources,  predicting negative values in at least half the bins in all three cases. It generally seems to predict the greatest count in the region around the average energy of each source, i.e. above, below and at 1 MeV for AmBe, AmLi and Cf respectively, as previous iterations of the model did. One possible way to combat this may be to introduce more complex sources into the training set, such as linear combinations of existing monoenergetics, to help the network learn how to better reconstruct a more complicated fluence.\n",
    "\n",
    "It is then informative to see the performance of the model on some of the validation data explicitly, as is shown below. The function used to plot this can be used to scroll through all of the training datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79942c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from NNPytorchLightning import FluenceReconDataset, Resample\n",
    "\n",
    "cubes_dataloader = torch.load('/home/nr1315/Documents/Project/MachineLearning/lightning_logs/model_cubes_new_data/version_1/val_dloader.pt')\n",
    "cubes_dataset,cubes_val_inds = cubes_dataloader.dataset.dataset,cubes_dataloader.dataset.indices\n",
    "\n",
    "check = NNPL.CheckTrainData(model_cubes,cubes_dataset,np.load(energy_bins))\n",
    "\n",
    "check.ViewTrainData()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548bce26",
   "metadata": {},
   "source": [
    "![550keV model cubes pred](Plots/model_cubes_550keV_pred.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f532f0",
   "metadata": {},
   "source": [
    "The model has generally performed well at predicting the bins for this validation data point with some count in an incorrect bin, but of note is that it is still predicting negative values in several bins. In order to avoid this, it may prove useful to add a term to the loss function that penalises any negative bins in the model prediction. \n",
    "\n",
    "A more thorough check of the validation data set for both this model and the following will be performed later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec887dc",
   "metadata": {},
   "source": [
    "### Cubes and profiles model, 200 samples per dataset, learning rate 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d33d5ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nr1315/miniconda3/envs/NewMLEnv/lib/python3.9/site-packages/scipy/interpolate/interpolate.py:623: RuntimeWarning: invalid value encountered in true_divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/nr1315/miniconda3/envs/NewMLEnv/lib/python3.9/site-packages/scipy/interpolate/interpolate.py:623: RuntimeWarning: invalid value encountered in true_divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n"
     ]
    }
   ],
   "source": [
    "NNPL.PlotLosses([model_cubes_profiles_tloss,model_cubes_profiles_vloss],['Training loss','Validation loss'],'Cubes and profiles model, lr = 0.001, 200 samples per dataset',model_cubes_profiles_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c15676b",
   "metadata": {},
   "source": [
    "![losses](Plots/model_cubes_profiles_200samples.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0ee180",
   "metadata": {},
   "source": [
    "Whilst both the training loss and validation loss do both decrease, it is worth noting that the validation loss here is lower than the training loss, which in turn implies that the model needs to train for longer in this configuration. This is intuitive, as the increased number of bins due to the profile counts will mean there are a greater number of weights for the model to optimize. Other improvements may be found from increasing the learning rate or adding some learning rate scheduling, but otherwise training for longer is realistically required.\n",
    "\n",
    "It is also worthy of note that the loss is higher than for the cubes-only model, but this may be a factor of the incomplete training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c20cae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,3,figsize=(30,10))\n",
    "\n",
    "l4 = NNPL.compare_pred_true(model_cubes_profiles,testing_data_dir+'withProfiles/'+AmBe_counts,testing_data_dir+AmBe_target,'AmBe','cubes and \\n profiles model',ax[0],24,np.load(energy_bins),1,loss_fn)\n",
    "l5 = NNPL.compare_pred_true(model_cubes_profiles,testing_data_dir+'withProfiles/'+AmLi_counts,testing_data_dir+AmLi_target,'AmLi','cubes and \\n profiles model',ax[1],24,np.load(energy_bins),1,loss_fn)\n",
    "l6 = NNPL.compare_pred_true(model_cubes_profiles,testing_data_dir+'withProfiles/'+Cf_counts,testing_data_dir+Cf_target,r'$^{252}$Cf','cubes and \\n profiles model',ax[2],24,np.load(energy_bins),1,loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c48bee",
   "metadata": {},
   "source": [
    "![cubes profiles 200 samples prediction](Plots/model_cubes_profiles_200samples_prediction.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6c1b932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AmBe loss: 0.09213663637638092\n",
      "AmLi loss: 0.05291319638490677\n",
      "Cf-252 loss: 0.04942954331636429\n"
     ]
    }
   ],
   "source": [
    "print(\"AmBe loss: {}\".format(l4))\n",
    "print(\"AmLi loss: {}\".format(l5))\n",
    "print(\"Cf-252 loss: {}\".format(l6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba312d44",
   "metadata": {},
   "source": [
    "This model also performs poorly on the unseen data, although it has a lower loss than the cubes exclusive model. Most notably, this model predicts a large negative value in the first bin for all three sources, which further motivates introducing a penalty term to the loss to discourage any negative values in the model output. \n",
    "\n",
    "\n",
    "Validation data set checking needs some more work before conclusions can be drawn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "377b0678",
   "metadata": {},
   "outputs": [],
   "source": [
    "cubes_profiles_dataloader = torch.load('/home/nr1315/Documents/Project/MachineLearning/lightning_logs/model_cubes_profiles_new_data/version_5/val_dloader.pt')\n",
    "cubes_profiles_dataset,cubes_profiles_val_inds = cubes_profiles_dataloader.dataset.dataset,cubes_profiles_dataloader.dataset.indices\n",
    "\n",
    "check = NNPL.CheckTrainData(model_cubes_profiles,cubes_profiles_dataset,np.load(energy_bins))\n",
    "\n",
    "check.ViewTrainData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aae42c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e496bb01",
   "metadata": {},
   "source": [
    "After these results both models were trained again for 2000 epochs, as it appeared that neither model had fully trained in the 500 epochs here. These models and the monitoring quantities are loaded in here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "59b0a3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cubes_2000 = NNPL.LoadModel('/home/nr1315/Documents/Project/MachineLearning/lightning_logs/model_cubes_new_data/version_4/',torch.rand((1,1,64)),coeffs,energy_bins)\n",
    "\n",
    "model_cubes_profiles_2000 = NNPL.LoadModel('/home/nr1315/Documents/Project/MachineLearning/lightning_logs/model_cubes_profiles_new_data/version_11/',torch.rand((1,1,76)),coeffs,energy_bins)\n",
    "\n",
    "model_cubes_2000_tloss = pd.read_csv('/home/nr1315/Documents/Project/MachineLearning/LoggedParameters/run-model_cubes_new_data_version_4-tag-train_loss.csv')\n",
    "model_cubes_2000_vloss = pd.read_csv('/home/nr1315/Documents/Project/MachineLearning/LoggedParameters/run-model_cubes_new_data_version_4-tag-val_loss.csv')\n",
    "model_cubes_2000_dose_err = pd.read_csv('/home/nr1315/Documents/Project/MachineLearning/LoggedParameters/run-model_cubes_new_data_version_4-tag-dose_err_AP.csv')\n",
    "model_cubes_2000_epoch = pd.read_csv('/home/nr1315/Documents/Project/MachineLearning/LoggedParameters/run-model_cubes_new_data_version_4-tag-epoch.csv')\n",
    "\n",
    "model_cubes_profiles_2000_tloss = pd.read_csv('/home/nr1315/Documents/Project/MachineLearning/LoggedParameters/run-model_cubes_profiles_new_data_version_11-tag-train_loss.csv')\n",
    "model_cubes_profiles_2000_vloss = pd.read_csv('/home/nr1315/Documents/Project/MachineLearning/LoggedParameters/run-model_cubes_profiles_new_data_version_11-tag-val_loss.csv')\n",
    "model_cubes_profiles_2000_dose_err = pd.read_csv('/home/nr1315/Documents/Project/MachineLearning/LoggedParameters/run-model_cubes_profiles_new_data_version_11-tag-dose_err_AP.csv')\n",
    "\n",
    "model_cubes_profiles_2000_epoch = pd.read_csv('/home/nr1315/Documents/Project/MachineLearning/LoggedParameters/run-model_cubes_profiles_new_data_version_11-tag-epoch.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db007d47",
   "metadata": {},
   "source": [
    "### Cubes model, learning rate 0.001, 200 samples per dataset, trained for 2000 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "67448e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "NNPL.PlotLosses([model_cubes_2000_tloss,model_cubes_2000_vloss],['Training loss','Validation loss'],'Cubes model, lr = 0.001, 200 samples per dataset, 2000 epochs',model_cubes_2000_epoch,log=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0864ef42",
   "metadata": {},
   "source": [
    "![model cubes 2000epochs loss](Plots/model_cubes_2000_loss.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6b7a4d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NNPL.PlotDoseError(model_cubes_2000_dose_err,'Cubes model, lr = 0.001, 200 samples per dataset, 2000 epochs, dose error',model_cubes_2000_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630b5532",
   "metadata": {},
   "source": [
    "![model cubes 2000 dose error](Plots/model_cubes_2000_dose_err.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b1d26b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,3,figsize=(30,10))\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "cubes_2000_AmBe_loss = NNPL.compare_pred_true(model_cubes_2000,testing_data_dir+AmBe_counts,testing_data_dir+AmBe_target,'AmBe','cubes only model,\\n 2000 epochs',ax[0],24,np.load(energy_bins),1,loss_fn)\n",
    "cubes_2000_AmLi_loss = NNPL.compare_pred_true(model_cubes_2000,testing_data_dir+AmLi_counts,testing_data_dir+AmLi_target,'AmLi','cubes only model,\\n 2000 epochs',ax[1],24,np.load(energy_bins),1,loss_fn)\n",
    "cubes_2000_Cf_loss = NNPL.compare_pred_true(model_cubes_2000,testing_data_dir+Cf_counts,testing_data_dir+Cf_target,r'$^{252}$Cf','cubes only model,\\n 2000 epochs',ax[2],24,np.load(energy_bins),1,loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ffcf5e",
   "metadata": {},
   "source": [
    "![cubes model 2000 predictions](Plots/model_cubes_2000_prediction_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fa730cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AmBe loss: 0.0924101173877716\n",
      "AmLi loss: 0.02471756935119629\n",
      "Cf-252 loss: 0.034533705562353134\n"
     ]
    }
   ],
   "source": [
    "print(\"AmBe loss: {}\".format(cubes_2000_AmBe_loss))\n",
    "print(\"AmLi loss: {}\".format(cubes_2000_AmLi_loss))\n",
    "print(\"Cf-252 loss: {}\".format(cubes_2000_Cf_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f713611e",
   "metadata": {},
   "source": [
    "### Cubes and profiles model, learning rate 0.001, 200 samples per dataset, trained for 2000 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b2f345a",
   "metadata": {},
   "outputs": [],
   "source": [
    "NNPL.PlotLosses([model_cubes_profiles_2000_tloss,model_cubes_profiles_2000_vloss],['Training loss','Validation loss'],'Cubes and profiles model, lr = 0.001, 200 samples per dataset, 2000 epochs',model_cubes_profiles_2000_epoch,log=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5569e771",
   "metadata": {},
   "source": [
    "![Cubes profiles 2000 epochs loss](Plots/model_cubes_profiles_2000epochs_loss.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "66c9334c",
   "metadata": {},
   "outputs": [],
   "source": [
    "NNPL.PlotDoseError(model_cubes_profiles_2000_dose_err,'Cubes and profiles model dose error',model_cubes_profiles_2000_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf721d8",
   "metadata": {},
   "source": [
    "![cubes profiles 2000 epochs dose error](Plots/model_cubes_profiles_2000epochs_dose_err.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9fd0dca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(1,3,figsize=(30,10))\n",
    "\n",
    "cubes_profiles_2000_AmBe_loss = NNPL.compare_pred_true(model_cubes_profiles_2000,testing_data_dir+'withProfiles/'+AmBe_counts,testing_data_dir+AmBe_target,'AmBe','cubes and \\n profiles model, 2000 epochs',ax[0],24,np.load(energy_bins),1,loss_fn)\n",
    "cubes_profiles_2000_AmLi_loss = NNPL.compare_pred_true(model_cubes_profiles_2000,testing_data_dir+'withProfiles/'+AmLi_counts,testing_data_dir+AmLi_target,'AmLi','cubes and \\n profiles model, 2000 epochs',ax[1],24,np.load(energy_bins),1,loss_fn)\n",
    "cubes_profiles_2000_Cf_loss = NNPL.compare_pred_true(model_cubes_profiles_2000,testing_data_dir+'withProfiles/'+Cf_counts,testing_data_dir+Cf_target,r'$^{252}$Cf','cubes and \\n profiles model, 2000 epochs',ax[2],24,np.load(energy_bins),1,loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78586d47",
   "metadata": {},
   "source": [
    "![cubes profiles 2000 predictions](Plots/model_cubes_profiles_2000_prediction.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4844a4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AmBe loss: 0.10277150571346283\n",
      "AmLi loss: 0.05674799904227257\n",
      "Cf-252 loss: 0.03377455845475197\n"
     ]
    }
   ],
   "source": [
    "print(\"AmBe loss: {}\".format(cubes_profiles_2000_AmBe_loss))\n",
    "print(\"AmLi loss: {}\".format(cubes_profiles_2000_AmLi_loss))\n",
    "print(\"Cf-252 loss: {}\".format(cubes_profiles_2000_Cf_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088c2546",
   "metadata": {},
   "source": [
    "### Current results & future steps\n",
    "\n",
    "Prediction on the test data is approximately equivalently bad for all of these models. All models seem to have learned to predict significant values in at most 2 or 3 bins, likely as the vase majority of the sources used for training only have counts in at most 3 bins. A way of tackling this would be to augment the training dataset with sources that are continuous across more bins than the existing data points, which could be accomplished in several ways:\n",
    "\n",
    "* Generating additional training data with a greater spread in energy (e.g. larger Gaussian spread), such that individual data points cover more energy bins\n",
    "* Implementing a transform to produce new data points composed of a linear combination of two or more existing data points\n",
    "* Training on 1 or 2 of the test sources, to see if that improves prediction on the third (and on more complex sources in general) \n",
    "\n",
    "Whilst the first approach is straightforward, it will add data points that peak around a particular bin and have a spread around that bin, rather than producing a more complex neutron spectrum that may have multiple peaks, although more complex distributions than a Gaussian could be used. It also would require generating a significant amount of additional training data, which is not storage efficient.\n",
    "\n",
    "The second approach can in principle can teach the model more flexibility in how sources with a wide range of energies are encoded in the counts measured by the detector. This transformation would allow the model to learn an how an arbitrary fluence is encoded in the inputs, but in order to result in good prediction on real sources it may require some more careful logic on which sources are combined by the transformation, e.g. only taking those close in energy. However, following implementation of a rotation transform it may be informative to provide training data that is a composite of a high energy source on one side of the detector with a thermal source on the opposite side, to emulate a scenario with backscattered thermal neutrons. \n",
    "\n",
    "Regarding the final approach eventually the model will be trained on all of these sources, but in the mean time training on one of these may be useful to test the ability of the model to learn a more continuous source on top of the monoenergetic sources it is already learning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe524592",
   "metadata": {},
   "source": [
    "The model also can regularly predicit negative values in energy bins, when obviously this is not a physical result. In order to prevent this, I propose adding a term to the loss that penalises any bins with a negative value in the output. This could for example be done with a simple boolean check on each value in the output tensor, adding a factor *k* to the loss for each bin that has a negative value. This factor *k* could be tuned at initialisation of the loss function in order to tune how harshly the model penalises negative bins, and the ideal value of this parameter can be investigated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c975e3",
   "metadata": {},
   "source": [
    "A final point of note is how many epochs the model takes to train - even after 2000 epochs the training and validation losses are still decreasing, along with the average absolute percentage error on dose. This motivates revisiting the optimisation of the network, particularly looking at the learning rate or a learning rate scheduler, as well as the choice of optimisation algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
